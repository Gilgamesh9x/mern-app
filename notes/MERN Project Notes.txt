-----MERN Project notes: 

1: To create favicons: favicon.io | https://undraw.co/

2: Installs for front: npm i react-router | npm i styled-components 

3: PS: when using styled components, the main css file works but when you add a style to a component specifically, it only works on that component (but scoped to this component)

4: PS: using styled components, if you have a component nested in the wrapper, that compnent will have access to the <Wrapper> styles

5: Very important note about React router. If you pass a prop to the Outlet, that prop will be available to all pages related to that Outlet

6: Icons for react: https://react-icons.github.io/react-icons/ => npm i react-icons

7: When using ES6 module type with node, we need to always include the extention .js when importing stuff

8: Packages we're using: 

npm install bcryptjs concurrently cookie-parser dayjs dotenv express express-async-errors express-validator http-status-codes jsonwebtoken mongoose morgan multer nanoid nodemon cloudinary dayjs datauri helmet express-rate-limit express-mongo-sanitize

9: We create a .gitignore and copy and paste the data from .gitignore of the client

10: add the script: "setup-project": "npm i && cd client && npm i"

11: We install the Thunder Client extension to test our API (it's like Postman)

12: Explanation of the middleware express.json()

----------------------------------------------------------------------------------------------------------------------------------------

1: Front End (JSON.stringify):

When you send data from the front end, you typically use JSON.stringify(eventData) to convert your JavaScript object (eventData) into a JSON string. This is because HTTP requests often send data as strings, and JSON is a standard format for sending structured data as a string.
The request is then sent to the backend with the data in JSON format.

2:Backend (express.json()):

On the backend, the middleware app.use(express.json()) is responsible for parsing incoming JSON data. It looks at the incoming request's body, and if it's in JSON format, it automatically converts (or "parses") the JSON string back into a regular JavaScript object.
This way, you can easily access and manipulate the data from req.body as a regular JavaScript object in your backend code.

So, the sequence is:

Frontend: Object → JSON (with JSON.stringify)
Backend: JSON → Object (with express.json())

--------------------------------------------------------------------------------------------------------------------------------------------

13: The reason why we added: 
NODE_ENV=development
PORT=5100

in the env file is because in the production both their values are gonna change (the development will change to production and the port will change the host's port)

14: New node features: 
	1: global await
	2: fetch
	3: --watch

15: npm i nanoid: used for generating random ids. id: nanoid()

16: database: medtajaoui | eNrHSG3wzQc41w7z

17: PS: if you send a value to the database that wasn't defined in the schema, it just gets ignored

18: Create secrets: in cmd: openssl rand -base64 32

19: When verifying JWT tokens, what we care about is "did we grant this cookie?" and we can tell if we did or not by the signature. So if we are the ones who granted the token, then we don't care about the data in it, we'll just let the user in.

20: HTTP Only Cookie:  (when this cookie is stored in the client, whenever we make a request to the API, that cookie is sent with the request). (So on the server, first, we grab the token from the cookie, then we decode it and then access the resources)

An HTTP-only cookie is a cookie that can't be accessed by JavaScript running in the browser. It is designed to help prevent cross-site scripting (XSS) attacks, which can be used to steal cookies and other sensitive information.

21: HTTP Only Cookie VS Local Storage

An HTTP-only cookie is a type of cookie that is designed to be inaccessible to JavaScript running in the browser. It is primarily used for authentication purposes and is a more secure way of storing sensitive information like user tokens. Local storage, on the other hand, is a browser-based storage mechanism that is accessible to JavaScript, and is used to store application data like preferences or user-generated content. While local storage is convenient, it is not a secure way of storing sensitive information as it can be accessed and modified by JavaScript running in the browser.

22: Why are we using a proxy and not just CORS? 

Using a proxy during development is another common approach to handle cross-origin issues, and it's often simpler than dealing with CORS and cookies directly.

'/api': This is the path to match. If a request is made to the development server with a path that starts with /api, the proxy rule will be applied.
target: 'http://localhost:5100/api': This specifies the target URL where the requests will be redirected. In this case, any request that matches the /api path will be forwarded to http://localhost:5100/api.

changeOrigin: true: When set to true, this property changes the origin of the request to match the target URL. This can be useful when working with CORS (Cross-Origin Resource Sharing) restrictions.

rewrite: (path) => path.replace(/^\/api/, ''): This property allows you to modify the path of the request before it is forwarded to the target. In this case, the rewrite function uses a regular expression (/^\/api/) to remove the /api prefix from the path. For example, if a request is made to /api/users, the rewritten path will be /users.

23: The concurrently npm package is a utility that allows you to run multiple commands concurrently in the same terminal window. It provides a convenient way to execute multiple tasks or processes simultaneously. (It will run both servers in the same terminal

24: Toastify: to show notifications

25:  with the outlet, we can pass any kind of data we want. Here we are passing an object user using: context={{ user }}

26: Note: When the form is submitted (using actions), React Router's default behavior is to revalidate all loaders on the current route. (meaning, it'll send any requests to get data related to that page)

27: npm i dayjs (helps us with formatting dates) | https://day.js.org/docs/en/display/format#docsNav

28: In order to upload images to our server, we needed to create a public folder in which we keep the assets we need for the frontend but we did it differently since we are using ES6 and not commonJS. Create a public folder in the root and then: 

import { dirname, path } from "path";
import { fileURLToPath } from "url";

const __dirname = dirname(fileURLToPath(import.meta.url));
app.use(express.static(path.resolve(__dirname, "./public")));

Now if we put a file in public, we can access it from http://localhost/5100/avatar.jpg

29: When we send a file to our server, we do it a bit differently. First, in the form, we add: <Form method="post" className="form" encType="multipart/form-data">. Then, in the action, we don't send json but rather the FormData (which is not json nor a JS object)

30: Multer is a popular middleware package for handling multipart/form-data in Node.js web applications. It is commonly used for handling file uploads. Multer simplifies the process of accepting and storing files submitted through HTTP requests by providing an easy-to-use API. It integrates seamlessly with Express.js and allows developers to define upload destinations, file size limits, and other configurations.

31: Cloudinary is a cloud-based media management platform that helps businesses store, optimize, and deliver images and videos across the web. It provides developers with an easy way to upload, manipulate, and serve media assets, enabling faster and more efficient delivery of visual content on websites and applications. Cloudinary also offers features like automatic resizing, format conversion, and responsive delivery to ensure optimal user experiences across different devices and network conditions.

So what we will do is, we will upload the image to cloudinary, and then use the URL that it provides us with to serve it to the frontend

32: To generate mock data: https://www.mockaroo.com/

33: The MongoDB aggregation pipeline is like a factory line for data. Data enters, it goes through different stages like cleaning, sorting, or grouping, and comes out at the end changed in some way. It's a way to process data inside MongoDB.

Why we used Mongo aggregate: 

Your approach of filtering the jobs array works perfectly fine, especially if you have a manageable number of jobs for each user. Using Array.prototype.filter() is straightforward and easy to read. However, there are some cases where using MongoDB's aggregation pipeline (like the instructor did with Job.aggregate) might be more efficient or necessary: Database Efficiency, Reduced Network Load, Scalability.

34: For charts in React: recharts.org

35: Very important note concerning the <Form> component. When we don't mention the method "post" in it, the browser automatically sends a get request with query params in the url having the values of whatever we filled in the form. And it triggers the loader related to that page
